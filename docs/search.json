[
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "STA 9750 Miniproject 1",
    "section": "",
    "text": "Netflix is one of the most popular streaming services in the word, with the company reporting approximately 90 million subscribers in Q4 2024. In this project, I will be analyzing the Top 10 Shows and Movies on Netflix at the given moment, and give commentary on the insights I found."
  },
  {
    "objectID": "mp01.html#which-tv-showseason-hit-the-top-10-in-the-most-countries-in-its-debut-week-in-how-many-countries-did-it-chart",
    "href": "mp01.html#which-tv-showseason-hit-the-top-10-in-the-most-countries-in-its-debut-week-in-how-many-countries-did-it-chart",
    "title": "STA 9750 Miniproject 1",
    "section": "#10: Which TV show/season hit the top 10 in the most countries in its debut week? In how many countries did it chart?",
    "text": "#10: Which TV show/season hit the top 10 in the most countries in its debut week? In how many countries did it chart?\n\ntv_data &lt;- COUNTRY_TOP_10 %&gt;% \n  filter(str_detect(category, \"TV\")) #Filter by TV Shows\n\n# Find the debut week for each TV show season\ntv_debuts &lt;- tv_data %&gt;% \n  group_by(show_title, season_title) %&gt;% \n  summarise(\n    debut_week = min(week, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n# Keep only rows corresponding to the debut week for each show-season\ntv_with_debut &lt;- tv_data %&gt;% \n  inner_join(tv_debuts, by = c(\"show_title\", \"season_title\", \"week\" = \"debut_week\")) # Joins original TV data with debut weeks: matches on show, season, and week = debut_week\n\n# Count how many countries each show-season appeared in during its debut week\ndebut_reach &lt;- tv_with_debut %&gt;% \n  group_by(show_title, season_title, week) %&gt;%\n  summarise(\n    num_countries = n_distinct(country_name),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(num_countries))\n\n# Identify the TV show-season that debuted in the most countries\ntop_debut &lt;- debut_reach %&gt;%  slice(1) # Take the top row (highest debut reach)\ntop_debut\n\n# A tibble: 1 × 4\n  show_title     season_title             week       num_countries\n  &lt;chr&gt;          &lt;chr&gt;                    &lt;date&gt;             &lt;int&gt;\n1 Emily in Paris Emily in Paris: Season 2 2021-12-26            94\n\n\n\nThe TV show that hit top 10 in the most countries in its debut week was Emily in Paris, debuting in all 94 countries Netflix is active in!"
  },
  {
    "objectID": "mp01.html#press-release-1-stranger-things-a-global-phenomenon-returns-for-its-final-season-in-2025",
    "href": "mp01.html#press-release-1-stranger-things-a-global-phenomenon-returns-for-its-final-season-in-2025",
    "title": "STA 9750 Miniproject 1",
    "section": "Press Release #1: Stranger Things: A Global Phenomenon Returns for its final season in 2025",
    "text": "Press Release #1: Stranger Things: A Global Phenomenon Returns for its final season in 2025\nNetflix’s global hit series Stranger Things will return in 2025 for its highly anticipated final season, marking the conclusion of one of the most successful and influential franchises in TV history.\n\n# 1. Filter Stranger Things from GLOBAL_TOP_10\nstranger_global &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(str_detect(show_title, \"Stranger Things\"))\n\n# Total global hours viewed across all seasons\nstranger_total_hours &lt;- stranger_global %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\n# Total number of weeks in Global Top 10\nstranger_total_weeks &lt;- stranger_global %&gt;%\n  summarise(total_weeks = n_distinct(week))\n\n# 2. Country-level analysis\nstranger_country &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(str_detect(show_title, \"Stranger Things\"))\n\n# Number of unique countries where Stranger Things appeared\nstranger_total_countries &lt;- stranger_country %&gt;%\n  summarise(num_countries = n_distinct(country_name))\n\n# 3. Compare to other English-language TV shows\nenglish_tv_global &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(str_detect(category, \"TV (English)\"))\n\ncomparison &lt;- english_tv_global %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(\n    total_hours = sum(weekly_hours_viewed, na.rm = TRUE),\n    total_weeks = n_distinct(week),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(total_hours))\n\n# Peek at top 10 shows for context\ntop10_english_tv &lt;- head(comparison, 10)\n\n# Combine the single-value summaries into one row\nstranger_summary &lt;- tibble(\n  total_hours = stranger_total_hours$total_hours,\n  total_weeks = stranger_total_weeks$total_weeks,\n  total_countries = stranger_total_countries$num_countries\n)\n\nstranger_summary\n\n# A tibble: 1 × 3\n  total_hours total_weeks total_countries\n        &lt;dbl&gt;       &lt;int&gt;           &lt;int&gt;\n1  2967980000          20              93\n\ntop10_english_tv\n\n# A tibble: 0 × 3\n# ℹ 3 variables: show_title &lt;chr&gt;, total_hours &lt;dbl&gt;, total_weeks &lt;int&gt;\n\n\n\nStranger Things comes back strong in 2025 with a highly-anticipated final season, with 296780000 hours being watched collectively across 4 seasons\nStranger Things experienced 20 weeks in the top 10\nStranger Things experienced global top 10 popularity in every single country Netflix is active in."
  },
  {
    "objectID": "mp01.html#press-release-2-netflix-in-india-record-breaking-growth-in-worlds-largest-country",
    "href": "mp01.html#press-release-2-netflix-in-india-record-breaking-growth-in-worlds-largest-country",
    "title": "STA 9750 Miniproject 1",
    "section": "Press Release #2: Netflix in India: Record-Breaking growth in World’s Largest Country",
    "text": "Press Release #2: Netflix in India: Record-Breaking growth in World’s Largest Country\nNetflix announced record-breaking growth in India, the world’s largest country, underscoring its expanding reach and strengthening its position in one of the fastest-growing entertainment markets.\n\n# Filter for Indian Top 10\nindia_top &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(country_name == \"India\")\n\nindia_with_global &lt;- india_top %&gt;%\n  inner_join(\n    GLOBAL_TOP_10 %&gt;% \n      select(show_title, runtime, `runtime`, weekly_hours_viewed, weekly_views, week),\n    by = c(\"show_title\", \"week\") # match by show title + week\n  )\n\nWarning in inner_join(., GLOBAL_TOP_10 %&gt;% select(show_title, runtime, runtime, : Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 53 of `x` matches multiple rows in `y`.\nℹ Row 111 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nlibrary(lubridate)\n\nindia_views_by_year &lt;- india_with_global %&gt;%\n  mutate(year = year(week)) %&gt;%   # extract the year from the week column\n  group_by(year) %&gt;%\n  summarise(\n    total_hours_viewed = sum(weekly_hours_viewed, na.rm = TRUE),\n    total_views = sum(weekly_views, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(year)\n\nindia_summary &lt;- india_with_global %&gt;%\n  group_by(show_title, category) %&gt;%\n  summarise(\n    total_hours_viewed = sum(weekly_hours_viewed, na.rm = TRUE),\n    total_weeks_in_top10 = n_distinct(week),\n    runtime_hours = max(runtime, na.rm = TRUE),   # runtime is already in hours\n    est_views = total_hours_viewed / runtime_hours,  # total hours / hours per view\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(total_hours_viewed))\n\nWarning: There were 316 warnings in `summarise()`.\nThe first warning was:\nℹ In argument: `runtime_hours = max(runtime, na.rm = TRUE)`.\nℹ In group 1: `show_title = \"'83\"` `category = \"Films\"`.\nCaused by warning in `max()`:\n! no non-missing arguments to max; returning -Inf\nℹ Run `dplyr::last_dplyr_warnings()` to see the 315 remaining warnings.\n\nindia_summary\n\n# A tibble: 801 × 6\n   show_title     category total_hours_viewed total_weeks_in_top10 runtime_hours\n   &lt;chr&gt;          &lt;chr&gt;                 &lt;dbl&gt;                &lt;int&gt;         &lt;dbl&gt;\n 1 Stranger Thin… TV              10576060000                   19       -Inf   \n 2 Squid Game     TV               8644700000                   40          8.32\n 3 Money Heist    TV               5398950000                   15          0   \n 4 Wednesday      TV               3948750000                   32          7.78\n 5 Bridgerton     TV               3665980000                   17          8.5 \n 6 Manifest       TV               2365870000                   17       -Inf   \n 7 Ginny & Georg… TV               2232200000                   10         10.6 \n 8 The Witcher    TV               1958670000                   14          8   \n 9 You            TV               1801700000                   14          8.45\n10 Sex Education  TV               1614820000                   12          8.1 \n# ℹ 791 more rows\n# ℹ 1 more variable: est_views &lt;dbl&gt;\n\nindia_views_by_year\n\n# A tibble: 5 × 3\n   year total_hours_viewed total_views\n  &lt;dbl&gt;              &lt;dbl&gt;       &lt;dbl&gt;\n1  2021        16997140000           0\n2  2022        33092540000           0\n3  2023        20614820000  2411800000\n4  2024        18266500000  4569200000\n5  2025        20167700000  4543000000\n\n\n\nNetflix’s presence in India has exploded since its deployment, seeing a massive surge in show viewership from 2.4 billion to 4.5 billion from 2023-2024\nNetflix saw its highest level of consumption of content in 2022 in India, with over 330 billion hours of content viewed\nWhile total viewership slightly declined between 2024-2025, Netflix’s Indian market is expected to remain stable in the long-run"
  },
  {
    "objectID": "mp01.html#press-release-3-a-celebration-of-success-for-korean-netflix-tv",
    "href": "mp01.html#press-release-3-a-celebration-of-success-for-korean-netflix-tv",
    "title": "STA 9750 Miniproject 1",
    "section": "Press Release 3: A Celebration of Success for Korean Netflix TV",
    "text": "Press Release 3: A Celebration of Success for Korean Netflix TV\nNetflix celebrates the success of its expansion to Korea, celebrating record viewership, critical acclaim, and the continued power of the Korean demographic as well as the demand for Korean content.\n\n# Filter for Korean-origin shows in Global Top 10\nkdrama_global &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(str_detect(country_name, \"Korea\"))\n\n# Join with GLOBAL_TOP_10 to get global hours/views info\nkdrama_combined &lt;- kdrama_global %&gt;%\n  inner_join(\n    GLOBAL_TOP_10 %&gt;%\n      select(show_title, week, weekly_hours_viewed, weekly_views, runtime, category),\n    by = c(\"show_title\", \"week\")\n  )\n\nWarning in inner_join(., GLOBAL_TOP_10 %&gt;% select(show_title, week, weekly_hours_viewed, : Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 80 of `x` matches multiple rows in `y`.\nℹ Row 421 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n# Filter only TV shows\nkdrama_tv &lt;- kdrama_combined %&gt;%\n  filter(str_detect(category.x, \"TV\"))  # keep only TV shows\n\n# Summarize top Korean TV shows globally\ntop_kdramas_tv &lt;- kdrama_tv %&gt;%\n  group_by(country_name, show_title) %&gt;%  # group by show title only\n  summarise(\n    total_hours = sum(weekly_hours_viewed, na.rm = TRUE),\n    total_views = sum(weekly_views, na.rm = TRUE),\n    total_weeks = n_distinct(week),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(country_name, desc(total_hours)) %&gt;%  # sort by total hours viewed\n  slice_head(n = 10)              # top 10\n\ntop_kdramas_tv\n\n# A tibble: 10 × 5\n   country_name show_title                 total_hours total_views total_weeks\n   &lt;chr&gt;        &lt;chr&gt;                            &lt;dbl&gt;       &lt;dbl&gt;       &lt;int&gt;\n 1 South Korea  Squid Game                  6955950000   683700000          27\n 2 South Korea  Stranger Things             2943100000           0           6\n 3 South Korea  Wednesday                   2571350000   221300000          11\n 4 South Korea  Bridgerton                  1660970000   165500000           8\n 5 South Korea  The Witcher                  753700000    36200000           5\n 6 South Korea  Emily in Paris               740670000    51800000           8\n 7 South Korea  Money Heist                  631600000           0           3\n 8 South Korea  Queen of Tears               617800000    41500000          13\n 9 South Korea  Extraordinary Attorney Woo   614980000           0          15\n10 South Korea  All of Us Are Dead           599270000           0           6\n\n\n\nTV Shows are taking Korea by storm — Squid Game alone has dazzled audiences with over 683 million views\nGlobal shows continue to rank up popularity in South Korea, from Extraordinary Attorney Woo to All of Us Are Dead, Korean audiences are spending millions of hours streaming their favorite TV hits\nSquid Game remains the most impactful show, having remained top 10 popularity status for 27 weeks in total\n\n\nConclusion:\nNetflix continues to be a prime contender as one of the world’s leading streaming services worldwide, with its portfolio still raking many hits such as Squid Game, KPop Demon Hunters, and Stranger Things. Notable, Korean-inspired content seems to be trending greatly, with Squid Game having being the single most viewed show on Netflix to this date and KPop Demon Hunters being the most viewed English film. If I were to make one recommendation from my insights, investing into Korean-inspired content is a strong strategy, as it aligns with audience viewing trends and will help maintain Netflix’s global reach in the long run."
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "STA 9750 Miniproject 2",
    "section": "",
    "text": "While NIMBYs fight to freeze neighborhoods in place, YIMBYs recognize that progress comes from building upward and outward. By analyzing national data on rent burden and housing growth, this week’s project demonstrates that expanding supply truly makes housing more affordable. The results make a compelling case for embracing the YIMBY approach.\n\nFirst, we will import data from the American Census Bureau\n\nif(!dir.exists(file.path(\"data\", \"mp02\"))){\n    dir.create(file.path(\"data\", \"mp02\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nlibrary &lt;- function(pkg){\n    ## Mask base::library() to automatically install packages if needed\n    ## Masking is important here so downlit picks up packages and links\n    ## to documentation\n    pkg &lt;- as.character(substitute(pkg))\n    options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n    if(!require(pkg, character.only=TRUE, quietly=TRUE)) install.packages(pkg)\n    stopifnot(require(pkg, character.only=TRUE, quietly=TRUE))\n}\n\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(readxl)\nlibrary(tidycensus)\n\nget_acs_all_years &lt;- function(variable, geography=\"cbsa\",\n                              start_year=2009, end_year=2023){\n    fname &lt;- glue(\"{variable}_{geography}_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        YEARS &lt;- seq(start_year, end_year)\n        YEARS &lt;- YEARS[YEARS != 2020] # Drop 2020 - No survey (covid)\n        \n        ALL_DATA &lt;- map(YEARS, function(yy){\n            tidycensus::get_acs(geography, variable, year=yy, survey=\"acs1\") |&gt;\n                mutate(year=yy) |&gt;\n                select(-moe, -variable) |&gt;\n                rename(!!variable := estimate)\n        }) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\n# Household income (12 month)\nINCOME &lt;- get_acs_all_years(\"B19013_001\") |&gt;\n    rename(household_income = B19013_001)\n\n# Monthly rent\nRENT &lt;- get_acs_all_years(\"B25064_001\") |&gt;\n    rename(monthly_rent = B25064_001)\n\n# Total population\nPOPULATION &lt;- get_acs_all_years(\"B01003_001\") |&gt;\n    rename(population = B01003_001)\n\n# Total number of households\nHOUSEHOLDS &lt;- get_acs_all_years(\"B11001_001\") |&gt;\n    rename(households = B11001_001)\n\n\n\nThen, we will add additional housing units from 2009-2023.\n\nget_building_permits &lt;- function(start_year = 2009, end_year = 2023){\n    fname &lt;- glue(\"housing_units_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        HISTORICAL_YEARS &lt;- seq(start_year, 2018)\n        \n        HISTORICAL_DATA &lt;- map(HISTORICAL_YEARS, function(yy){\n            historical_url &lt;- glue(\"https://www.census.gov/construction/bps/txt/tb3u{yy}.txt\")\n                \n            LINES &lt;- readLines(historical_url)[-c(1:11)]\n\n            CBSA_LINES &lt;- str_detect(LINES, \"^[[:digit:]]\")\n            CBSA &lt;- as.integer(str_sub(LINES[CBSA_LINES], 5, 10))\n\n            PERMIT_LINES &lt;- str_detect(str_sub(LINES, 48, 53), \"[[:digit:]]\")\n            PERMITS &lt;- as.integer(str_sub(LINES[PERMIT_LINES], 48, 53))\n            \n            data_frame(CBSA = CBSA,\n                       new_housing_units_permitted = PERMITS, \n                       year = yy)\n        }) |&gt; bind_rows()\n        \n        CURRENT_YEARS &lt;- seq(2019, end_year)\n        \n        CURRENT_DATA &lt;- map(CURRENT_YEARS, function(yy){\n            current_url &lt;- glue(\"https://www.census.gov/construction/bps/xls/msaannual_{yy}99.xls\")\n            \n            temp &lt;- tempfile()\n            \n            download.file(current_url, destfile = temp, mode=\"wb\")\n            \n            fallback &lt;- function(.f1, .f2){\n                function(...){\n                    tryCatch(.f1(...), \n                             error=function(e) .f2(...))\n                }\n            }\n            \n            reader &lt;- fallback(read_xlsx, read_xls)\n            \n            reader(temp, skip=5) |&gt;\n                na.omit() |&gt;\n                select(CBSA, Total) |&gt;\n                mutate(year = yy) |&gt;\n                rename(new_housing_units_permitted = Total)\n        }) |&gt; bind_rows()\n        \n        ALL_DATA &lt;- rbind(HISTORICAL_DATA, CURRENT_DATA)\n        \n        write_csv(ALL_DATA, fname)\n        \n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\nPERMITS &lt;- get_building_permits()\n\n\n\nThen, we will get NACIS Data\n\nlibrary(httr2)\nlibrary(rvest)\nget_bls_industry_codes &lt;- function(){\n    fname &lt;- fname &lt;- file.path(\"data\", \"mp02\", \"bls_industry_codes.csv\")\n    \n    if(!file.exists(fname)){\n    \n        resp &lt;- request(\"https://www.bls.gov\") |&gt; \n            req_url_path(\"cew\", \"classifications\", \"industry\", \"industry-titles.htm\") |&gt;\n            req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n            req_error(is_error = \\(resp) FALSE) |&gt;\n            req_perform()\n        \n        resp_check_status(resp)\n        \n        naics_table &lt;- resp_body_html(resp) |&gt;\n            html_element(\"#naics_titles\") |&gt; \n            html_table() |&gt;\n            mutate(title = str_trim(str_remove(str_remove(`Industry Title`, Code), \"NAICS\"))) |&gt;\n            select(-`Industry Title`) |&gt;\n            mutate(depth = if_else(nchar(Code) &lt;= 5, nchar(Code) - 1, NA)) |&gt;\n            filter(!is.na(depth))\n        \n        naics_table &lt;- naics_table |&gt; \n            filter(depth == 4) |&gt; \n            rename(level4_title=title) |&gt; \n            mutate(level1_code = str_sub(Code, end=2), \n                   level2_code = str_sub(Code, end=3), \n                   level3_code = str_sub(Code, end=4)) |&gt;\n            left_join(naics_table, join_by(level1_code == Code)) |&gt;\n            rename(level1_title=title) |&gt;\n            left_join(naics_table, join_by(level2_code == Code)) |&gt;\n            rename(level2_title=title) |&gt;\n            left_join(naics_table, join_by(level3_code == Code)) |&gt;\n            rename(level3_title=title) |&gt;\n            select(-starts_with(\"depth\")) |&gt;\n            rename(level4_code = Code) |&gt;\n            select(level1_title, level2_title, level3_title, level4_title, \n                   level1_code,  level2_code,  level3_code,  level4_code)\n    \n        write_csv(naics_table, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n    \n}\n\nINDUSTRY_CODES &lt;- get_bls_industry_codes()\n\n\n\nFinally, we scrape BLS Quarterly Census of Employment and Wages\n\nlibrary(httr2)\nlibrary(rvest)\nget_bls_qcew_annual_averages &lt;- function(start_year=2009, end_year=2023){\n    fname &lt;- glue(\"bls_qcew_{start_year}_{end_year}.csv.gz\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    YEARS &lt;- seq(start_year, end_year)\n    YEARS &lt;- YEARS[YEARS != 2020] # Drop Covid year to match ACS\n    \n    if(!file.exists(fname)){\n        ALL_DATA &lt;- map(YEARS, .progress=TRUE, possibly(function(yy){\n            fname_inner &lt;- file.path(\"data\", \"mp02\", glue(\"{yy}_qcew_annual_singlefile.zip\"))\n            \n            if(!file.exists(fname_inner)){\n                request(\"https://www.bls.gov\") |&gt; \n                    req_url_path(\"cew\", \"data\", \"files\", yy, \"csv\",\n                                 glue(\"{yy}_annual_singlefile.zip\")) |&gt;\n                    req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n                    req_retry(max_tries=5) |&gt;\n                    req_perform(fname_inner)\n            }\n            \n            if(file.info(fname_inner)$size &lt; 755e5){\n                warning(sQuote(fname_inner), \"appears corrupted. Please delete and retry this step.\")\n            }\n            \n            read_csv(fname_inner, \n                     show_col_types=FALSE) |&gt; \n                mutate(YEAR = yy) |&gt;\n                select(area_fips, \n                       industry_code, \n                       annual_avg_emplvl, \n                       total_annual_wages, \n                       YEAR) |&gt;\n                filter(nchar(industry_code) &lt;= 5, \n                       str_starts(area_fips, \"C\")) |&gt;\n                filter(str_detect(industry_code, \"-\", negate=TRUE)) |&gt;\n                mutate(FIPS = area_fips, \n                       INDUSTRY = as.integer(industry_code), \n                       EMPLOYMENT = as.integer(annual_avg_emplvl), \n                       TOTAL_WAGES = total_annual_wages) |&gt;\n                select(-area_fips, \n                       -industry_code, \n                       -annual_avg_emplvl, \n                       -total_annual_wages) |&gt;\n                # 10 is a special value: \"all industries\" , so omit\n                filter(INDUSTRY != 10) |&gt; \n                mutate(AVG_WAGE = TOTAL_WAGES / EMPLOYMENT)\n        })) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    ALL_DATA &lt;- read_csv(fname, show_col_types=FALSE)\n    \n    ALL_DATA_YEARS &lt;- unique(ALL_DATA$YEAR)\n    \n    YEARS_DIFF &lt;- setdiff(YEARS, ALL_DATA_YEARS)\n    \n    if(length(YEARS_DIFF) &gt; 0){\n        stop(\"Download failed for the following years: \", YEARS_DIFF, \n             \". Please delete intermediate files and try again.\")\n    }\n    \n    ALL_DATA\n}\n\nWAGES &lt;- get_bls_qcew_annual_averages()\n\n\n\nThe following questions will explore the housing data and help familiarize the overarching issue\n\nWhich CBSA (by name) permitted the largest number of new housing units in the decade from 2010 to 2019 (inclusive)?\n\n#Step 1: Create query permits_summary and filter years 2010-2019\npermits_summary &lt;- PERMITS %&gt;%\n  filter(year &gt;= 2010, year &lt;= 2019) %&gt;%\n  group_by(CBSA) %&gt;%\n  summarise(total_units = sum(new_housing_units_permitted, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# Step 2: Prepare crosswalk from ACS data\ncbsa_xwalk &lt;- INCOME %&gt;%\n  select(GEOID, NAME) %&gt;%\n  distinct() %&gt;%\n  mutate(CBSA = as.integer(GEOID))\n\n# Step 3: Join to get CBSA names\npermits_with_names &lt;- permits_summary %&gt;%\n  left_join(cbsa_xwalk, by = \"CBSA\") %&gt;%\n  arrange(desc(total_units))\n\n# Step 4: Show the top result\ntop_cbsa &lt;- permits_with_names %&gt;% slice(1)\ntop_cbsa\n\n# A tibble: 1 × 4\n   CBSA total_units GEOID NAME                                     \n  &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                                    \n1 26420      482075 26420 Houston-Sugar Land-Baytown, TX Metro Area\n\n\n\nHouston Sugar Land Baytown had the highest amount of new housing units from 2010-2019!\n#2: In what year did Albuquerque, NM, permit the most new housing units?\n\nalbuquerque_trend &lt;- PERMITS %&gt;%\n  filter(CBSA == 10740) %&gt;%\n  group_by(year) %&gt;%\n  summarise(total_units = sum(new_housing_units_permitted, na.rm = TRUE)) %&gt;%\n  arrange(desc(total_units))\n\n# View the trend\nalbuquerque_trend\n\n# A tibble: 15 × 2\n    year total_units\n   &lt;dbl&gt;       &lt;dbl&gt;\n 1  2021        4021\n 2  2022        2852\n 3  2023        2834\n 4  2013        2606\n 5  2014        2543\n 6  2016        2465\n 7  2015        2295\n 8  2017        2256\n 9  2018        2186\n10  2019        2148\n11  2012        2084\n12  2020        2014\n13  2010        1764\n14  2009        1692\n15  2011        1634\n\npeak_year &lt;- albuquerque_trend %&gt;% slice(1)\npeak_year\n\n# A tibble: 1 × 2\n   year total_units\n  &lt;dbl&gt;       &lt;dbl&gt;\n1  2021        4021\n\n\n2021 was the year with the highest amount of housing units, possibly due to low interest rates and a post-COVID backlog of construction demand.\n#3: Which state had the highest average individual income in 2015?\n\nlibrary(dplyr)\nlibrary(stringr)\n\n# Step 1: Join ACS datasets by GEOID and year\nacs_combined &lt;- INCOME %&gt;%\n  inner_join(HOUSEHOLDS, by = c(\"GEOID\", \"NAME\", \"year\")) %&gt;%\n  inner_join(POPULATION, by = c(\"GEOID\", \"NAME\", \"year\"))\n\n# Step 2: Filter for 2015\nacs_2015 &lt;- acs_combined %&gt;%\n  filter(year == 2015)\n\n# Step 3: Compute total income per CBSA\nacs_2015 &lt;- acs_2015 %&gt;%\n  mutate(total_income = household_income * households)\n\n# Step 4: Extract the *principal* state (first two-letter code in CBSA name)\nacs_2015 &lt;- acs_2015 %&gt;%\n  mutate(state = str_extract(NAME, \", (.{2})\", group = 1))\n\n# Step 5: Summarize total income and total population per state\nstate_summary &lt;- acs_2015 %&gt;%\n  group_by(state) %&gt;%\n  summarise(\n    total_income = sum(total_income, na.rm = TRUE),\n    total_population = sum(population, na.rm = TRUE),\n    avg_individual_income = total_income / total_population\n  ) %&gt;%\n  arrange(desc(avg_individual_income))\n\n# Step 6: Get top state\ntop_state &lt;- state_summary %&gt;% slice(1)\ntop_state\n\n# A tibble: 1 × 4\n  state total_income total_population avg_individual_income\n  &lt;chr&gt;        &lt;dbl&gt;            &lt;dbl&gt;                 &lt;dbl&gt;\n1 DC    202663489140          6098283                33233.\n\n\nDC was the “State” with the highest average individual income, signifying a high concentration of high-income sectors such as federal agencies, consulting firms, etc.\n\nWhat was the last year in which the NYC CBSA had the most data scientists in the country?\n\n\nlibrary(dplyr)\nlibrary(stringr)\n\n# Step 1: Prepare a CBSA name lookup from ACS data\ncbsa_lookup &lt;- INCOME %&gt;%\n  select(GEOID, NAME) %&gt;%\n  distinct() %&gt;%\n  mutate(std_cbsa = paste0(\"C\", GEOID))  # Census convention: prefix 'C'\n\n# Step 2: Filter BLS data for NAICS 5182 (Data Scientists & Analysts)\ndata_sci &lt;- WAGES %&gt;%\n  filter(INDUSTRY == 5182) %&gt;%\n  mutate(std_cbsa = paste0(FIPS, \"0\"))   # BLS convention: suffix '0'\n\n# Step 3: Join BLS to CBSA names\ndata_sci_named &lt;- suppressWarnings(\n  data_sci %&gt;%\n    inner_join(cbsa_lookup, join_by(std_cbsa == std_cbsa))\n)\n\n# Step 4: Find which CBSA had the most data scientists each year\ncbsa_top_by_year &lt;- data_sci_named %&gt;%\n  group_by(YEAR, NAME) %&gt;%\n  summarise(total_employment = sum(EMPLOYMENT, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  arrange(YEAR, desc(total_employment)) %&gt;%\n  group_by(YEAR) %&gt;%\n  slice(1) %&gt;%\n  ungroup()\n\n# Step 5: Identify the last year NYC was #1\nnyc_last_top &lt;- cbsa_top_by_year %&gt;%\n  filter(str_detect(NAME, \"New York\")) %&gt;%\n  summarise(last_year_top = max(YEAR))\n\nnyc_last_top\n\n# A tibble: 1 × 1\n  last_year_top\n          &lt;dbl&gt;\n1          2015\n\n\n2015 was the last year where NYC CBSA had the highest amount of data scientists\n#5: What fraction of total wages in the NYC CBSA was earned by people employed in the finance and insurance industries (NAICS code 52)? In what year did this fraction peak?\n\nlibrary(dplyr)\nlibrary(stringr)\n\n# Step 1: Create standardized CBSA code for joining\nwages_std &lt;- WAGES %&gt;%\n  mutate(std_cbsa = paste0(FIPS, \"0\"))  # add trailing 0 to match Census convention\n\n# Step 2: Identify NYC CBSA\nnyc_cbsa &lt;- \"35620\"\nnyc_std_cbsa &lt;- paste0(\"C\", nyc_cbsa)\n\n# Step 3: Filter for NYC CBSA only\nnyc_wages &lt;- wages_std %&gt;%\n  filter(std_cbsa == paste0(\"C\", nyc_cbsa) | FIPS == nyc_cbsa | FIPS == \"35620\")  # flexible match\n\n# Step 4: Compute total wages and finance wages per year\nnyc_summary &lt;- nyc_wages %&gt;%\n  group_by(YEAR) %&gt;%\n  summarise(\n    total_wages = sum(TOTAL_WAGES, na.rm = TRUE),\n    finance_wages = sum(TOTAL_WAGES[INDUSTRY == 52 | str_starts(as.character(INDUSTRY), \"52\")],\n                        na.rm = TRUE)\n  ) %&gt;%\n  mutate(finance_fraction = finance_wages / total_wages) %&gt;%\n  arrange(YEAR)\n\n# Step 5: Identify peak year\npeak_year &lt;- nyc_summary %&gt;%\n  filter(finance_fraction == max(finance_fraction, na.rm = TRUE))\n\nnyc_summary\n\n# A tibble: 14 × 4\n    YEAR   total_wages finance_wages finance_fraction\n   &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;\n 1  2009 2194200174111  206223026063           0.0940\n 2  2010 2366455158958  363922560518           0.154 \n 3  2011 2452875238158  337460139179           0.138 \n 4  2012 2543568475824  361225542250           0.142 \n 5  2013 2663041532424  316780948682           0.119 \n 6  2014 2587096519796  368544350266           0.142 \n 7  2015 3008221071967  383349052749           0.127 \n 8  2016 3038312046515  352181610994           0.116 \n 9  2017 3176952501734  436667758453           0.137 \n10  2018 3196707717054  429401942724           0.134 \n11  2019 3617150803059  489522399195           0.135 \n12  2021 3636399927489  577101733960           0.159 \n13  2022 4104601808975  559385567580           0.136 \n14  2023 4160135177511  497713155936           0.120 \n\npeak_year\n\n# A tibble: 1 × 4\n   YEAR   total_wages finance_wages finance_fraction\n  &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;\n1  2021 3636399927489  577101733960            0.159\n\n\n15.8% of total wages were earned by people in the Finance and Insurance industries. And that number peaked in 2021!\n\n\nWe will create several visualizations using the data provided!\n\nMonthly rent vs average household income (circa 2009)\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\n\n\n#Filter income to 2009\nrent_income_2009 &lt;- RENT %&gt;%\n  filter(year == 2009) %&gt;%\n  select(GEOID, NAME, monthly_rent) %&gt;%\n  inner_join(\n    INCOME %&gt;% filter(year == 2009) %&gt;% select(GEOID, household_income),\n    by = \"GEOID\"\n  )\n\n#Create the graph\nggplot(rent_income_2009, aes(x = household_income, y = monthly_rent)) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE, linewidth = 1) +\n  scale_x_continuous(labels = label_dollar(accuracy = 1)) +\n  scale_y_continuous(labels = label_dollar(accuracy = 1)) +\n  labs(\n    title = \"Monthly Rent vs. Household Income (CBSA, 2009)\",\n    x = \"Median Household Income (USD)\",\n    y = \"Median Monthly Rent (USD)\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nWe can see in the graph that median household income has a positive correlation with median rent!\n\nWhat is the relationship between total employment and and total employment in the healthcare and social services sector(NACIS 62) across different CBSAs.\n\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(scales)\n\n# Aggregate from WAGES\nemp_by_cbsa_year &lt;- WAGES %&gt;%\n  group_by(FIPS, YEAR) %&gt;%\n  summarise(\n    total_emp = sum(EMPLOYMENT, na.rm = TRUE),\n    health_emp = sum(EMPLOYMENT[str_starts(as.character(INDUSTRY), \"62\")], na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n# Optional: add CBSA names for tooltips/labels (not necessary for the plot)\ncbsa_names &lt;- INCOME %&gt;%\n  select(GEOID, NAME) %&gt;%\n  distinct() %&gt;%\n  mutate(FIPS = paste0(\"C\", GEOID))   # WAGES uses 'C#####' for CBSA\n\nemp_by_cbsa_year &lt;- emp_by_cbsa_year %&gt;%\n  left_join(cbsa_names, by = \"FIPS\")\n\n# Faceted scatter by year (evolution over time)\nggplot(emp_by_cbsa_year, aes(x = total_emp, y = health_emp)) +\n  geom_point(alpha = 0.6) +\n  facet_wrap(~ YEAR, ncol = 4) +\n  scale_x_continuous(labels = label_comma()) +\n  scale_y_continuous(labels = label_comma()) +\n  labs(\n    title = \"Health Care & Social Assistance Employment vs. Total Employment by CBSA\",\n    subtitle = \"NAICS 62; small multiples show evolution over time\",\n    x = \"Total Employment (all industries)\",\n    y = \"Employment in Health Care & Social Assistance (NAICS 62)\"\n  ) +\n  theme_minimal(base_size = 12.5) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\nWhat is the evolution of average household size over time?\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\n\n# Compute average household size per CBSA & year\nhhsize &lt;- POPULATION %&gt;%\n  select(GEOID, NAME, year, population) %&gt;%\n  inner_join(\n    HOUSEHOLDS %&gt;% select(GEOID, year, households),\n    by = c(\"GEOID\", \"year\")\n  ) %&gt;%\n  mutate(avg_hh_size = population / households)\n\n# Choose top N CBSAs by population in a reference year for clarity\nN &lt;- 10\ntop_cbsa &lt;- hhsize %&gt;%\n  filter(year == 2015) %&gt;%\n  arrange(desc(population)) %&gt;%\n  slice_head(n = N) %&gt;%\n  pull(GEOID)\n\nhhsize_top &lt;- hhsize %&gt;% filter(GEOID %in% top_cbsa)\nhhsize_top &lt;- hhsize_top %&gt;%\n  mutate(\n    # Normalize all dash types to a single hyphen first\n    NAME = str_replace_all(NAME, \"[–—]\", \"-\"),\n    # Remove everything after the first hyphen (keeps first city)\n    NAME = str_replace(NAME, \"-.*\", \"\"),\n    # Trim any trailing spaces\n    NAME = str_trim(NAME)\n  )\n\nggplot(hhsize_top, aes(x = year, y = avg_hh_size, color = NAME, group = NAME)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 1.5) +\n  scale_y_continuous(labels = label_number(accuracy = 0.01)) +\n  labs(\n    title = \"Evolution of Average Household Size\",\n    subtitle = paste0(\"Top \", N, \" CBSAs by population (reference: 2015)\"),\n    x = \"Year\",\n    y = \"Average Household Size (persons per household)\",\n    color = \"CBSA\"\n  ) +\n  theme_minimal(base_size = 12.5) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    legend.position = \"bottom\",\n    legend.key.width = unit(1.4, \"lines\")\n  )\n\n\n\n\n\n\n\n\n\nInterestingly, the average household size has went down over the years for ALL CBSAs overall, with no one trend showing anything going up. While some household sizes in CBSAs like Washington and Miami went up from 2012-2016, the overall trend lays down.\n\n\nNow, we will join together the Income and Rent tables, and with the data construct a suitable measure of how much income a typical resident spends on housing.\n\nlibrary(dplyr)\n\nrent_income &lt;- RENT %&gt;%\n  inner_join(INCOME, by = c(\"GEOID\", \"NAME\", \"year\")) %&gt;%\n  mutate(\n    rent_to_income = (monthly_rent * 12) / household_income\n  )\n\n# Establish baseline (national average in 2009)\nbaseline_2009 &lt;- rent_income %&gt;%\n  filter(year == 2009) %&gt;%\n  summarise(baseline = mean(rent_to_income, na.rm = TRUE)) %&gt;%\n  pull(baseline)\n\n# Create standardized index\nrent_income &lt;- rent_income %&gt;%\n  mutate(\n    rent_burden_index = (rent_to_income / baseline_2009) * 100\n  )\n\nlibrary(DT)\n\nmetro_name &lt;- \"New York-Newark-Jersey City, NY-NJ-PA Metro Area\"\n\nmetro_table &lt;- rent_income %&gt;%\n  filter(NAME == metro_name) %&gt;%\n  select(NAME, year, monthly_rent, household_income,\n         rent_to_income, rent_burden_index) %&gt;%\n  arrange(year)\n\ndatatable(\n  metro_table,\n  caption = paste0(\"Rent Burden Evolution for \", metro_name),\n  options = list(pageLength = 10, autoWidth = TRUE, scrollX = TRUE),\n  rownames = FALSE\n)\n\n\n\n\n# Latest year in your data (or pick one, e.g., 2023)\nlatest_year &lt;- max(rent_income$year, na.rm = TRUE)\n\ntop_bottom &lt;- rent_income %&gt;%\n  filter(year == latest_year) %&gt;%\n  arrange(desc(rent_burden_index)) %&gt;%\n  select(NAME, year, rent_burden_index, rent_to_income, monthly_rent, household_income)\n\ntop10 &lt;- head(top_bottom, 10)\nbottom10 &lt;- tail(top_bottom, 10)\n\ndatatable(top10, caption = paste0(\"Top 10 CBSAs by Rent Burden (\", latest_year, \")\"),\n          options = list(pageLength = 10, autoWidth = TRUE, scrollX = TRUE), rownames = FALSE)\n\n\n\n\ndatatable(bottom10, caption = paste0(\"Lowest 10 CBSAs by Rent Burden (\", latest_year, \")\"),\n          options = list(pageLength = 10, autoWidth = TRUE, scrollX = TRUE), rownames = FALSE)\n\n\n\n\n\nThe following code standardizes\n*We define the Rent Burden Index (RBI) so that 100 corresponds to the national average rent-to-income ratio in 2009, the first year of our study.\n\n\nNow its time to take a look at housing growth!\n\nlibrary(dplyr)\n\n# Assume the tables are named POPULATION and PERMITS\n# POPULATION: CBSA, YEAR, POP\n# PERMITS: CBSA, YEAR, PERMITS\n\n# Join using GEOID = CBSA and year = year\ncbsa_data &lt;- inner_join(POPULATION, PERMITS, \n                       by = c(\"GEOID\" = \"CBSA\", \"year\" = \"year\"))\n\n#Calculate rolling 5 year Population Growth\ncbsa_data &lt;- cbsa_data %&gt;%\n  arrange(GEOID, year) %&gt;%\n  group_by(GEOID) %&gt;%\n  mutate(\n    POP_5YRS_AGO = lag(population, 5),\n    POP_GROWTH_5YRS = population - POP_5YRS_AGO\n  ) %&gt;%\n  ungroup()\n\n#Construct your metric\ncbsa_data &lt;- cbsa_data %&gt;%\n  mutate(\n    # Instantaneous metric: permits per 1,000 people\n    PERMIT_PER_1000 = (new_housing_units_permitted / population) * 1000,\n    \n    # Rate-based metric: permits per 1,000 new residents (5-year growth)\n    PERMIT_PER_POPGROWTH = ifelse(!is.na(POP_GROWTH_5YRS) & POP_GROWTH_5YRS &gt; 0, \n                                  (new_housing_units_permitted / POP_GROWTH_5YRS) * 1000, NA))\n\n\n#Standardize each metric\ncbsa_data &lt;- cbsa_data %&gt;%\n  group_by(year) %&gt;%\n  mutate(\n    PERMIT_PER_1000_STD = (PERMIT_PER_1000 - min(PERMIT_PER_1000, na.rm=TRUE)) /\n                          (max(PERMIT_PER_1000, na.rm=TRUE) - min(PERMIT_PER_1000, na.rm=TRUE)),\n    PERMIT_PER_POPGROWTH_STD = (PERMIT_PER_POPGROWTH - min(PERMIT_PER_POPGROWTH, na.rm=TRUE)) /\n                               (max(PERMIT_PER_POPGROWTH, na.rm=TRUE) - min(PERMIT_PER_POPGROWTH, na.rm=TRUE))\n  ) %&gt;%\n  ungroup()\n\n#Identify high and law CBSAs\nlatest_year &lt;- max(cbsa_data$year, na.rm=TRUE)\n\ncat(\"\\n=== Top 5 CBSAs: Instantaneous Metric ===\\n\")\ntop5_instant &lt;- cbsa_data %&gt;%\n  filter(year == latest_year) %&gt;%\n  arrange(desc(PERMIT_PER_1000_STD)) %&gt;%\n  slice_head(n = 5) # Top 5 instantaneous\n\n\ncat(\"\\n=== Bottom 5 CBSAs: Instantaneous Metric ===\\n\")\nbottom5_instant &lt;- cbsa_data %&gt;%\n  filter(year == latest_year) %&gt;%\n  arrange(PERMIT_PER_1000_STD) %&gt;%\n  slice_head(n = 5)\n\n\n# Top 5 CBSAs by rate-based metric (permits per pop growth)\ncat(\"\\n=== Top 5 CBSAs: Rate-based Metric ===\\n\")\ntop5_rate_Based &lt;- cbsa_data %&gt;%\n  filter(year == latest_year) %&gt;%\n  arrange(desc(PERMIT_PER_POPGROWTH_STD)) %&gt;%\n  slice_head(n = 5)\n\n\n# Bottom 5 CBSAs by rate-based metric\ncat(\"\\n=== Bottom 5 CBSAs: Rate-based Metric ===\\n\")\nbottom5_rate_based &lt;- cbsa_data %&gt;%\n  filter(year == latest_year) %&gt;%\n  arrange(PERMIT_PER_POPGROWTH_STD) %&gt;%\n  slice_head(n = 5)\n\n\n#Composite score (based on equal-weight averages)\ncbsa_data &lt;- cbsa_data %&gt;%\n  mutate(\n    COMPOSITE_SCORE = (PERMIT_PER_1000_STD + PERMIT_PER_POPGROWTH_STD) / 2\n  )\n\n# Top 5 CBSAs by composite score\ncat(\"\\n=== Top 5 CBSAs: Composite Score ===\\n\")\ntop5_composite &lt;- cbsa_data %&gt;%\n  filter(year == latest_year) %&gt;%\n  arrange(desc(COMPOSITE_SCORE)) %&gt;%\n  slice_head(n = 5) \n\n\n# Bottom 5 CBSAs by composite score\ncat(\"\\n=== Bottom 5 CBSAs: Composite Score ===\\n\")\nbottom5_composite &lt;- cbsa_data %&gt;%\n  filter(year == latest_year) %&gt;%\n  arrange(COMPOSITE_SCORE) %&gt;%\n  slice_head(n = 5)\n\n\nTop 5 Instant (measured in PERMIT_PER_1000_STD)\n\ntop5_instant\n\n# A tibble: 5 × 11\n  GEOID NAME                population  year new_housing_units_pe…¹ POP_5YRS_AGO\n  &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt; &lt;dbl&gt;                  &lt;dbl&gt;        &lt;dbl&gt;\n1 41540 Salisbury, MD Metr…     129710  2023                   4894       405853\n2 34820 Myrtle Beach-Conwa…     397478  2023                  13176       464165\n3 39460 Punta Gorda, FL Me…     206134  2023                   4429       182033\n4 18880 Crestview-Fort Wal…     304818  2023                   5596       271346\n5 35840 North Port-Bradent…     910108  2023                  15928       804690\n# ℹ abbreviated name: ¹​new_housing_units_permitted\n# ℹ 5 more variables: POP_GROWTH_5YRS &lt;dbl&gt;, PERMIT_PER_1000 &lt;dbl&gt;,\n#   PERMIT_PER_POPGROWTH &lt;dbl&gt;, PERMIT_PER_1000_STD &lt;dbl&gt;,\n#   PERMIT_PER_POPGROWTH_STD &lt;dbl&gt;\n\n\nThe Salisbury, MD Metro Area had the highest housing units permitted per 1000 people!\n\n\nBottom 5 Instant\n\nbottom5_instant\n\n# A tibble: 5 × 11\n  GEOID NAME                population  year new_housing_units_pe…¹ POP_5YRS_AGO\n  &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt; &lt;dbl&gt;                  &lt;dbl&gt;        &lt;dbl&gt;\n1 48540 Wheeling, WV-OH Me…     135517  2023                     11       141254\n2 19180 Danville, IL Micro…      71652  2023                      8        77909\n3 34060 Morgantown, WV Met…     141817  2023                     17       138709\n4 48260 Weirton-Steubenvil…     114106  2023                     16       119664\n5 21420 Enid, OK Metro Area      62023  2023                     10        61581\n# ℹ abbreviated name: ¹​new_housing_units_permitted\n# ℹ 5 more variables: POP_GROWTH_5YRS &lt;dbl&gt;, PERMIT_PER_1000 &lt;dbl&gt;,\n#   PERMIT_PER_POPGROWTH &lt;dbl&gt;, PERMIT_PER_1000_STD &lt;dbl&gt;,\n#   PERMIT_PER_POPGROWTH_STD &lt;dbl&gt;\n\n\nWheeling, WV had the lowest amount of housing units permitted per 1000 people…\n\n\nTop 5 Rate-Based (PERMIT_PER_POPGROWTH_STD)\n\ntop5_rate_Based\n\n# A tibble: 5 × 11\n  GEOID NAME                population  year new_housing_units_pe…¹ POP_5YRS_AGO\n  &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt; &lt;dbl&gt;                  &lt;dbl&gt;        &lt;dbl&gt;\n1 44220 Springfield, OH Me…     134610  2023                    215       134557\n2 46520 Urban Honolulu, HI…     989408  2023                   1851       988650\n3 19140 Dalton, GA Metro A…     144722  2023                    496       144440\n4 15260 Brunswick-St. Simo…     115087  2023                    786       114473\n5 11260 Anchorage, AK Metr…     401314  2023                    457       400888\n# ℹ abbreviated name: ¹​new_housing_units_permitted\n# ℹ 5 more variables: POP_GROWTH_5YRS &lt;dbl&gt;, PERMIT_PER_1000 &lt;dbl&gt;,\n#   PERMIT_PER_POPGROWTH &lt;dbl&gt;, PERMIT_PER_1000_STD &lt;dbl&gt;,\n#   PERMIT_PER_POPGROWTH_STD &lt;dbl&gt;\n\n\nGoing by a rate-based metric, Springfield, OH had the highest housing permits issued per 1000 people.\n\n\nBottom 5 Rate-Based\n\nbottom5_rate_based\n\n# A tibble: 5 × 11\n  GEOID NAME                population  year new_housing_units_pe…¹ POP_5YRS_AGO\n  &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt; &lt;dbl&gt;                  &lt;dbl&gt;        &lt;dbl&gt;\n1 12100 Atlantic City-Hamm…     369823  2023                    383       269918\n2 30980 Longview, TX Metro…     293498  2023                    364       217481\n3 34060 Morgantown, WV Met…     141817  2023                     17       138709\n4 31740 Manhattan, KS Metr…     132831  2023                    257        98080\n5 27180 Jackson, TN Metro …     181826  2023                    434       129235\n# ℹ abbreviated name: ¹​new_housing_units_permitted\n# ℹ 5 more variables: POP_GROWTH_5YRS &lt;dbl&gt;, PERMIT_PER_1000 &lt;dbl&gt;,\n#   PERMIT_PER_POPGROWTH &lt;dbl&gt;, PERMIT_PER_1000_STD &lt;dbl&gt;,\n#   PERMIT_PER_POPGROWTH_STD &lt;dbl&gt;\n\n\nAtlantic City had the lowest housing permits issued per 1000 people.\n\n\nTop 5 Composite (measured in COMPOSITE_SCORE)\n\ntop5_composite\n\n# A tibble: 5 × 12\n  GEOID NAME                population  year new_housing_units_pe…¹ POP_5YRS_AGO\n  &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt; &lt;dbl&gt;                  &lt;dbl&gt;        &lt;dbl&gt;\n1 44220 Springfield, OH Me…     134610  2023                    215       134557\n2 46520 Urban Honolulu, HI…     989408  2023                   1851       988650\n3 39460 Punta Gorda, FL Me…     206134  2023                   4429       182033\n4 18880 Crestview-Fort Wal…     304818  2023                   5596       271346\n5 19140 Dalton, GA Metro A…     144722  2023                    496       144440\n# ℹ abbreviated name: ¹​new_housing_units_permitted\n# ℹ 6 more variables: POP_GROWTH_5YRS &lt;dbl&gt;, PERMIT_PER_1000 &lt;dbl&gt;,\n#   PERMIT_PER_POPGROWTH &lt;dbl&gt;, PERMIT_PER_1000_STD &lt;dbl&gt;,\n#   PERMIT_PER_POPGROWTH_STD &lt;dbl&gt;, COMPOSITE_SCORE &lt;dbl&gt;\n\n\nMeasuring with a composite score ((Instant Measure + Rate Measure) - 2), Springfield, OH, once again, has the highest amount of units issued per 1000 people!\n\n\nBottom 5 Composite\n\nbottom5_composite\n\n# A tibble: 5 × 12\n  GEOID NAME                population  year new_housing_units_pe…¹ POP_5YRS_AGO\n  &lt;dbl&gt; &lt;chr&gt;                    &lt;dbl&gt; &lt;dbl&gt;                  &lt;dbl&gt;        &lt;dbl&gt;\n1 34060 Morgantown, WV Met…     141817  2023                     17       138709\n2 21420 Enid, OK Metro Area      62023  2023                     10        61581\n3 11500 Anniston-Oxford, A…     116429  2023                     64       114728\n4 39740 Reading, PA Metro …     432821  2023                    352       417854\n5 26580 Huntington-Ashland…     367192  2023                    290       356474\n# ℹ abbreviated name: ¹​new_housing_units_permitted\n# ℹ 6 more variables: POP_GROWTH_5YRS &lt;dbl&gt;, PERMIT_PER_1000 &lt;dbl&gt;,\n#   PERMIT_PER_POPGROWTH &lt;dbl&gt;, PERMIT_PER_1000_STD &lt;dbl&gt;,\n#   PERMIT_PER_POPGROWTH_STD &lt;dbl&gt;, COMPOSITE_SCORE &lt;dbl&gt;\n\n\nMorgantown, WV had the least amount of housing units permitted per 1000 people.\n\n\nFinally, we will investigate the relationships between Rent Burden and Housing Growth\nPopulation Growth vs Housing Growth across CBSAs\nlibrary(ggplot2)\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Calculate cumulative population growth per CBSA over the whole period\ngrowth_stats &lt;- cbsa_data %&gt;%\n  group_by(GEOID, NAME) %&gt;%\n  summarize(\n    pop_start = population[year == min(year, na.rm=TRUE)],\n    pop_end = population[year == max(year, na.rm=TRUE)],\n    pop_growth = pop_end - pop_start,\n    composite_growth = mean(COMPOSITE_SCORE, na.rm=TRUE)\n  )\n\nlibrary(ggplot2)\n\nggplot(growth_stats, aes(\n  x = composite_growth,\n  y = pop_growth\n)) +\n  geom_point(color = \"steelblue\") +\n  labs(\n    title = \"Population Growth vs Housing Growth across CBSAs\",\n    x = \"Mean Composite Housing Growth (Standardized)\",\n    y = \"Population Growth (2009 to latest year)\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\n\n\n\nNow it’s time to identify the most YIMBY CBSAs!\n\nlibrary(dplyr)\n# Only need GEOID and year to join (assuming monthly_rent is the field you want)\ncbsa_full &lt;- left_join(cbsa_data, RENT %&gt;% select(GEOID, year, monthly_rent), by = c(\"GEOID\", \"year\"))\n\nsummary_burden &lt;- cbsa_full %&gt;%\n  group_by(GEOID, NAME) %&gt;%\n  summarize(\n    rent_start = monthly_rent[year == min(year, na.rm=TRUE)],\n    rent_end = monthly_rent[year == max(year, na.rm=TRUE)],\n    delta_rent = rent_end - rent_start,\n    pop_start = population[year == min(year, na.rm=TRUE)],\n    pop_end = population[year == max(year, na.rm=TRUE)],\n    pop_growth = pop_end - pop_start,\n    composite_growth = mean(COMPOSITE_SCORE, na.rm=TRUE)\n  )\n\n`summarise()` has grouped output by 'GEOID'. You can override using the\n`.groups` argument.\n\nlibrary(ggplot2)\n\n# Plot rent trend colored by above/below average housing growth\nsummary_burden &lt;- summary_burden %&gt;%\n  mutate(above_avg_growth = composite_growth &gt; mean(composite_growth, na.rm=TRUE))\n\ncbsa_full &lt;- cbsa_full %&gt;%\n  mutate(above_avg_growth = COMPOSITE_SCORE &gt; mean(COMPOSITE_SCORE, na.rm = TRUE))\n\nggplot(cbsa_full, aes(x = year, y = monthly_rent, color = above_avg_growth)) +\n  geom_line(aes(group = GEOID), alpha = 0.1) +\n  stat_summary(fun = mean, geom = \"line\", size = 1.3) +\n  labs(title = \"Average Rent Trends by Housing Growth Category\",\n       x = \"Year\", y = \"Average Monthly Rent\",\n       color = \"Above Avg Housing Growth\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\n\nggplot(summary_burden, aes(\n  x = composite_growth,\n  y = delta_rent,\n  size = pop_growth,\n  color = above_avg_growth\n)) +\n  geom_point(alpha = 0.7) +\n  labs(\n    title = \"Change in Rent vs Housing Growth\",\n    x = \"Mean Housing Growth\",\n    y = \"Change in Monthly Rent\",\n    size = \"Population Growth\",\n    color = \"Above Avg Growth\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    legend.position = \"right\"\n  )\n\nWarning: Removed 121 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nyimby_cbsas &lt;- summary_burden %&gt;%\n  filter(\n    rent_start &gt; quantile(rent_start, 0.75, na.rm=TRUE), # High starting rent\n    delta_rent &lt; 0,                                      # Rent decreased\n    pop_growth &gt; 0,                                      # Population increased\n    composite_growth &gt; mean(composite_growth, na.rm=TRUE) # Above avg housing growth\n  ) %&gt;%\n  arrange(delta_rent)\n\nyimby_cbsas %&gt;% select(GEOID, NAME, rent_start, delta_rent, pop_growth, composite_growth)\n\n# A tibble: 0 × 6\n# Groups:   GEOID [0]\n# ℹ 6 variables: GEOID &lt;dbl&gt;, NAME &lt;chr&gt;, rent_start &lt;dbl&gt;, delta_rent &lt;dbl&gt;,\n#   pop_growth &lt;dbl&gt;, composite_growth &lt;dbl&gt;\n\n#Function for checking the criteria for a \"YIMBY\" city\nsummary_burden &lt;- summary_burden %&gt;%\n  mutate(\n    crit_rent = rent_start &gt; quantile(rent_start, 0.75, na.rm=TRUE),\n    crit_delta = delta_rent &lt; 0,\n    crit_pop = pop_growth &gt; 0,\n    crit_growth = composite_growth &gt; mean(composite_growth, na.rm=TRUE),\n    num_criteria_met = crit_rent + crit_delta + crit_pop + crit_growth\n  )\n\n\nprint(summary_burden %&gt;% \n  filter(num_criteria_met &gt;= 3))\n\n# A tibble: 17 × 15\n# Groups:   GEOID [17]\n   GEOID NAME        rent_start rent_end delta_rent pop_start pop_end pop_growth\n   &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 10540 Albany-Leb…       1034     1134        100    129749  130467        718\n 2 12420 Austin-Rou…       1327     1599        272   2227083 2421115     194032\n 3 12540 Bakersfiel…        839     1208        369    807407  916108     108701\n 4 13460 Bend-Redmo…        900     1186        286    165954  191996      26042\n 5 14740 Bremerton-…       1433     1808        375    271473  277658       6185\n 6 21660 Eugene, OR…        863      998        135    356212  379611      23399\n 7 22660 Fort Colli…        986     1595        609    315988  366778      50790\n 8 25540 Hartford-E…       1113     1268        155   1204877 1221303      16426\n 9 28700 Kingsport-…        661      810        149    307707  312490       4783\n10 31420 Macon-Bibb…        743     1022        279    226998  233020       6022\n11 33100 Miami-Fort…       1120     1914        794   5828191 6183199     355008\n12 34740 Muskegon, …        657      993        336    171008  176565       5557\n13 35840 North Port…        958     1609        651    732535  891411     158876\n14 39580 Raleigh, N…        908     1128        220   1214516 1362540     148024\n15 42020 San Luis O…       1203     1430        227    276443  284010       7567\n16 44700 Stockton, …        998     1608        610    674860  793229     118369\n17 48300 Wenatchee,…        784     1294        510    113438  124118      10680\n# ℹ 7 more variables: composite_growth &lt;dbl&gt;, above_avg_growth &lt;lgl&gt;,\n#   crit_rent &lt;lgl&gt;, crit_delta &lt;lgl&gt;, crit_pop &lt;lgl&gt;, crit_growth &lt;lgl&gt;,\n#   num_criteria_met &lt;int&gt;\n\n#No city met all 4 criteria for a YIMBY city\nsummary_burden %&gt;% \n  filter(num_criteria_met == 4) \n\n# A tibble: 0 × 15\n# Groups:   GEOID [0]\n# ℹ 15 variables: GEOID &lt;dbl&gt;, NAME &lt;chr&gt;, rent_start &lt;dbl&gt;, rent_end &lt;dbl&gt;,\n#   delta_rent &lt;dbl&gt;, pop_start &lt;dbl&gt;, pop_end &lt;dbl&gt;, pop_growth &lt;dbl&gt;,\n#   composite_growth &lt;dbl&gt;, above_avg_growth &lt;lgl&gt;, crit_rent &lt;lgl&gt;,\n#   crit_delta &lt;lgl&gt;, crit_pop &lt;lgl&gt;, crit_growth &lt;lgl&gt;, num_criteria_met &lt;int&gt;\n\n\nThe line chart shows that metros with above-average housing growth (blue) actually experienced faster rent increases over time than those with below-average growth (red), suggesting that strong construction activity often coincides with high-demand, high-cost markets where both housing supply and rent levels rise together.\nThe scatterplot shows that metros with stronger housing growth generally experienced smaller or more moderate rent increases, while regions with limited construction saw rents rise faster — suggesting that sustained housing production can help ease rent pressures even in growing markets.\nUnfortunately there were no CBSAs that fit all 4 of the criteria to indicate an ideal YIMBY. Most notably, none of them have a negative delta, suggesting that an increase in rent across the U.S. is universal.\n\n\n\nPolicy Brief\nAmerica’s cities are currently facing housing affordability challenges. Some, like Bakersfield, CA, have tackled rent burdens through a combination of building homes fast enough to support population growth and keep rent burdens manageable. Many other cities, especially NYC, face high rents and housing shortages that threaten economic viability and workforce stability.\nLegislative Sponsors:\n\nPrimary Sponsor: Representative from Bakersfield, CA\n\nBakersfield is a YIMBY success story: more homes built, a growing population, and rent growth under control.\n\nCo-Sponsor: Representative from New York City, NY\n\nNew York City typifies the NIMBY (“Not-In-My-Back-Yard”) challenge, with persistently high rents and insufficient new housing construction.\n\n\nKey Support Groups:\nTo maximize impact and secure passage, the bill is tailored to win support from:\n\nTeachers and healthcare workers: Thousands of these essential professionals work in both metro areas, with healthcare and teaching representing Bakersville’s two largest sectors of employment. Lowering rents enhances their quality of life and keeps vital services staffed.\nRestaurant and entertainment industry workers: When residents spend less on rent, they have more discretionary income to dine out, attend shows, and enjoy the city’s cultural life—directly increasing demand for restaurants, live performances, and hospitality services\n\nPolicy Impact:\n\nBakersfield will be rewarded for policies that keep housing affordable and population growing, securing grants for further expansion and economic opportunity.\nNew York City stands to benefit most, with resources and incentives to jump-start a new era of housing growth, reduce rent burden, and attract and retain talent and industry.\n\nPerformance Metrics:\n\nRent Burden: We track whether average rent is increasing or decreasing over time—making sure cities are affordable for families and workers.\nHousing Growth: We monitor the rate of new home construction compared to population growth—ensuring enough homes for everyone.\n\nSummary:\nThe proposed bill offers cities a proven path to affordability and growth. It supports essential workers, strengthens the local economy, and channels resources where effective policies deliver real results. Bakersfield’s leadership shows what’s possible—now is the time to help cities like New York unlock similar success.\nSources:\n“Bakersfield, CA.” Data USA, datausa.io/profile/geo/bakersfield-ca/. Accessed 30 Oct. 2025."
  },
  {
    "objectID": "mini project 2 preparation code.html",
    "href": "mini project 2 preparation code.html",
    "title": "Sample",
    "section": "",
    "text": "if(!dir.exists(file.path(“data”, “mp02”))){ dir.create(file.path(“data”, “mp02”), showWarnings=FALSE, recursive=TRUE) }\nlibrary &lt;- function(pkg){ ## Mask base::library() to automatically install packages if needed ## Masking is important here so downlit picks up packages and links ## to documentation pkg &lt;- as.character(substitute(pkg)) options(repos = c(CRAN = “https://cloud.r-project.org”)) if(!require(pkg, character.only=TRUE, quietly=TRUE)) install.packages(pkg) stopifnot(require(pkg, character.only=TRUE, quietly=TRUE)) }\nlibrary(tidyverse) library(glue) library(readxl) library(tidycensus)\nget_acs_all_years &lt;- function(variable, geography=“cbsa”, start_year=2009, end_year=2023){ fname &lt;- glue(“{variable}{geography}{start_year}_{end_year}.csv”) fname &lt;- file.path(“data”, “mp02”, fname)\nif(!file.exists(fname)){\n    YEARS &lt;- seq(start_year, end_year)\n    YEARS &lt;- YEARS[YEARS != 2020] # Drop 2020 - No survey (covid)\n    \n    ALL_DATA &lt;- map(YEARS, function(yy){\n        tidycensus::get_acs(geography, variable, year=yy, survey=\"acs1\") |&gt;\n            mutate(year=yy) |&gt;\n            select(-moe, -variable) |&gt;\n            rename(!!variable := estimate)\n    }) |&gt; bind_rows()\n    \n    write_csv(ALL_DATA, fname)\n}\n\nread_csv(fname, show_col_types=FALSE)\n}\n\nHousehold income (12 month)\nINCOME &lt;- get_acs_all_years(“B19013_001”) |&gt; rename(household_income = B19013_001)\n\n\nMonthly rent\nRENT &lt;- get_acs_all_years(“B25064_001”) |&gt; rename(monthly_rent = B25064_001)\n\n\nTotal population\nPOPULATION &lt;- get_acs_all_years(“B01003_001”) |&gt; rename(population = B01003_001)\n\n\nTotal number of households\nHOUSEHOLDS &lt;- get_acs_all_years(“B11001_001”) |&gt; rename(households = B11001_001)\nget_building_permits &lt;- function(start_year = 2009, end_year = 2023){ fname &lt;- glue(“housing_units_{start_year}_{end_year}.csv”) fname &lt;- file.path(“data”, “mp02”, fname)\nif(!file.exists(fname)){\n    HISTORICAL_YEARS &lt;- seq(start_year, 2018)\n    \n    HISTORICAL_DATA &lt;- map(HISTORICAL_YEARS, function(yy){\n        historical_url &lt;- glue(\"https://www.census.gov/construction/bps/txt/tb3u{yy}.txt\")\n            \n        LINES &lt;- readLines(historical_url)[-c(1:11)]\n\n        CBSA_LINES &lt;- str_detect(LINES, \"^[[:digit:]]\")\n        CBSA &lt;- as.integer(str_sub(LINES[CBSA_LINES], 5, 10))\n\n        PERMIT_LINES &lt;- str_detect(str_sub(LINES, 48, 53), \"[[:digit:]]\")\n        PERMITS &lt;- as.integer(str_sub(LINES[PERMIT_LINES], 48, 53))\n        \n        data_frame(CBSA = CBSA,\n                   new_housing_units_permitted = PERMITS, \n                   year = yy)\n    }) |&gt; bind_rows()\n    \n    CURRENT_YEARS &lt;- seq(2019, end_year)\n    \n    CURRENT_DATA &lt;- map(CURRENT_YEARS, function(yy){\n        current_url &lt;- glue(\"https://www.census.gov/construction/bps/xls/msaannual_{yy}99.xls\")\n        \n        temp &lt;- tempfile()\n        \n        download.file(current_url, destfile = temp, mode=\"wb\")\n        \n        fallback &lt;- function(.f1, .f2){\n            function(...){\n                tryCatch(.f1(...), \n                         error=function(e) .f2(...))\n            }\n        }\n        \n        reader &lt;- fallback(read_xlsx, read_xls)\n        \n        reader(temp, skip=5) |&gt;\n            na.omit() |&gt;\n            select(CBSA, Total) |&gt;\n            mutate(year = yy) |&gt;\n            rename(new_housing_units_permitted = Total)\n    }) |&gt; bind_rows()\n    \n    ALL_DATA &lt;- rbind(HISTORICAL_DATA, CURRENT_DATA)\n    \n    write_csv(ALL_DATA, fname)\n    \n}\n\nread_csv(fname, show_col_types=FALSE)\n}\nPERMITS &lt;- get_building_permits()\nlibrary(httr2) library(rvest) get_bls_industry_codes &lt;- function(){ fname &lt;- file.path(“data”, “mp02”, “bls_industry_codes.csv”) library(dplyr) library(tidyr) library(readr)\nif(!file.exists(fname)){\n    \n    resp &lt;- request(\"https://www.bls.gov\") |&gt; \n        req_url_path(\"cew\", \"classifications\", \"industry\", \"industry-titles.htm\") |&gt;\n        req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n        req_error(is_error = \\(resp) FALSE) |&gt;\n        req_perform()\n    \n    resp_check_status(resp)\n    \n    naics_table &lt;- resp_body_html(resp) |&gt;\n        html_element(\"#naics_titles\") |&gt; \n        html_table() |&gt;\n        mutate(title = str_trim(str_remove(str_remove(`Industry Title`, Code), \"NAICS\"))) |&gt;\n        select(-`Industry Title`) |&gt;\n        mutate(depth = if_else(nchar(Code) &lt;= 5, nchar(Code) - 1, NA)) |&gt;\n        filter(!is.na(depth))\n    \n    # These were looked up manually on bls.gov after finding \n    # they were presented as ranges. Since there are only three\n    # it was easier to manually handle than to special-case everything else\n    naics_missing &lt;- tibble::tribble(\n        ~Code, ~title, ~depth, \n        \"31\", \"Manufacturing\", 1,\n        \"32\", \"Manufacturing\", 1,\n        \"33\", \"Manufacturing\", 1,\n        \"44\", \"Retail\", 1, \n        \"45\", \"Retail\", 1,\n        \"48\", \"Transportation and Warehousing\", 1, \n        \"49\", \"Transportation and Warehousing\", 1\n    )\n    \n    naics_table &lt;- bind_rows(naics_table, naics_missing)\n    \n    naics_table &lt;- naics_table |&gt; \n        filter(depth == 4) |&gt; \n        rename(level4_title=title) |&gt; \n        mutate(level1_code = str_sub(Code, end=2), \n               level2_code = str_sub(Code, end=3), \n               level3_code = str_sub(Code, end=4)) |&gt;\n        left_join(naics_table, join_by(level1_code == Code)) |&gt;\n        rename(level1_title=title) |&gt;\n        left_join(naics_table, join_by(level2_code == Code)) |&gt;\n        rename(level2_title=title) |&gt;\n        left_join(naics_table, join_by(level3_code == Code)) |&gt;\n        rename(level3_title=title) |&gt;\n        select(-starts_with(\"depth\")) |&gt;\n        rename(level4_code = Code) |&gt;\n        select(level1_title, level2_title, level3_title, level4_title, \n               level1_code,  level2_code,  level3_code,  level4_code) |&gt;\n        drop_na() |&gt;\n        mutate(across(contains(\"code\"), as.integer))\n    \n    write_csv(naics_table, fname)\n}\n\nread_csv(fname, show_col_types=FALSE)\n}\nINDUSTRY_CODES &lt;- get_bls_industry_codes()\nlibrary(httr2) library(rvest) get_bls_qcew_annual_averages &lt;- function(start_year=2009, end_year=2023){ fname &lt;- glue(“bls_qcew_{start_year}_{end_year}.csv.gz”) fname &lt;- file.path(“data”, “mp02”, fname)\nYEARS &lt;- seq(start_year, end_year)\nYEARS &lt;- YEARS[YEARS != 2020] # Drop Covid year to match ACS\n\nif(!file.exists(fname)){\n    ALL_DATA &lt;- map(YEARS, .progress=TRUE, possibly(function(yy){\n        fname_inner &lt;- file.path(\"data\", \"mp02\", glue(\"{yy}_qcew_annual_singlefile.zip\"))\n        \n        if(!file.exists(fname_inner)){\n            request(\"https://www.bls.gov\") |&gt; \n                req_url_path(\"cew\", \"data\", \"files\", yy, \"csv\",\n                             glue(\"{yy}_annual_singlefile.zip\")) |&gt;\n                req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n                req_retry(max_tries=5) |&gt;\n                req_perform(fname_inner)\n        }\n        \n        if(file.info(fname_inner)$size &lt; 755e5){\n            warning(sQuote(fname_inner), \"appears corrupted. Please delete and retry this step.\")\n        }\n        \n        read_csv(fname_inner, \n                 show_col_types=FALSE) |&gt; \n            mutate(YEAR = yy) |&gt;\n            select(area_fips, \n                   industry_code, \n                   annual_avg_emplvl, \n                   total_annual_wages, \n                   YEAR) |&gt;\n            filter(nchar(industry_code) &lt;= 5, \n                   str_starts(area_fips, \"C\")) |&gt;\n            filter(str_detect(industry_code, \"-\", negate=TRUE)) |&gt;\n            mutate(FIPS = area_fips, \n                   INDUSTRY = as.integer(industry_code), \n                   EMPLOYMENT = as.integer(annual_avg_emplvl), \n                   TOTAL_WAGES = total_annual_wages) |&gt;\n            select(-area_fips, \n                   -industry_code, \n                   -annual_avg_emplvl, \n                   -total_annual_wages) |&gt;\n            # 10 is a special value: \"all industries\" , so omit\n            filter(INDUSTRY != 10) |&gt; \n            mutate(AVG_WAGE = TOTAL_WAGES / EMPLOYMENT)\n    })) |&gt; bind_rows()\n    \n    write_csv(ALL_DATA, fname)\n}\n\nALL_DATA &lt;- read_csv(fname, show_col_types=FALSE)\n\nALL_DATA_YEARS &lt;- unique(ALL_DATA$YEAR)\n\nYEARS_DIFF &lt;- setdiff(YEARS, ALL_DATA_YEARS)\n\nif(length(YEARS_DIFF) &gt; 0){\n    stop(\"Download failed for the following years: \", YEARS_DIFF, \n         \". Please delete intermediate files and try again.\")\n}\n\nALL_DATA\n}\nWAGES &lt;- get_bls_qcew_annual_averages()"
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "STA 9750 Miniproject 3",
    "section": "",
    "text": "Trees are a critical part of New York City’s urban infrastructure—improving air quality, reducing heat, boosting neighborhood appeal, and supporting public health. Yet, not all city neighborhoods benefit equally from these advantages. In this project, I analyze street tree conditions across multiple districts, using geospatial data to highlight where the need for investment is greatest. By focusing on dead and missing trees, my goal is to make a clear, data-driven case for a targeted replanting initiative in District 23—showing how government can use analytics to maximize the impact of limited resources and deliver tangible improvements at the neighborhood level."
  },
  {
    "objectID": "mp03.html#task-4-district-level-analysis-of-tree-coverage",
    "href": "mp03.html#task-4-district-level-analysis-of-tree-coverage",
    "title": "STA 9750 Miniproject 3",
    "section": "Task 4: District-Level Analysis of Tree Coverage",
    "text": "Task 4: District-Level Analysis of Tree Coverage\nNow it is time to analyze our data to answer some questions!\n\nWhich council district has the most trees?\n\n\n\nCode\ntree_counts &lt;- tree_with_district %&gt;%\n  group_by(CounDist) %&gt;% \n  summarise(n_trees = n()) %&gt;%\n  arrange(desc(n_trees))\n\ntree_counts\n\n\nSimple feature collection with 51 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: -74.25448 ymin: 40.49891 xmax: -73.70139 ymax: 40.91292\nGeodetic CRS:  WGS 84\n# A tibble: 51 × 3\n   CounDist n_trees                                                     geometry\n      &lt;int&gt;   &lt;int&gt;                                             &lt;MULTIPOINT [°]&gt;\n 1       50    5313 ((-74.0244 40.61755), (-74.02614 40.61519), (-74.02539 40.6…\n 2       11    3354 ((-73.90238 40.91281), (-73.9024 40.91272), (-73.90243 40.9…\n 3       10    2943 ((-73.91238 40.87039), (-73.91211 40.87056), (-73.91199 40.…\n 4       51    2270 ((-74.14452 40.53772), (-74.14365 40.54), (-74.1445 40.5389…\n 5       49    2151 ((-74.13039 40.60395), (-74.1303 40.60395), (-74.13013 40.6…\n 6       33    2106 ((-73.95961 40.73231), (-73.95963 40.73247), (-73.95952 40.…\n 7       48    1782 ((-73.93525 40.5853), (-73.93529 40.58552), (-73.9342 40.58…\n 8       44    1693 ((-73.97463 40.59374), (-73.97286 40.59235), (-73.97256 40.…\n 9       46    1392 ((-73.9066 40.59553), (-73.90675 40.59566), (-73.9069 40.59…\n10        3    1309 ((-73.996 40.76749), (-73.99591 40.76761), (-73.99586 40.76…\n# ℹ 41 more rows\n\n\nCode\n# To get district with most trees:\ntop_district &lt;- tree_counts %&gt;% slice(1)\n\n\nDistrict 50 has the most graphed trees at 5313!\n2. Which district had the highest density of trees?\n\n\nCode\nlibrary(sf)\nlibrary(dplyr)\n\n# After counting trees per district (sf object tree_counts):\ntree_counts_no_geom &lt;- tree_counts %&gt;% st_drop_geometry()\n\n# district_areas is your districts sf object, select relevant columns then drop geometry:\narea_table &lt;- nycc_transformed %&gt;% \n  select(CounDist, Shape_Area) %&gt;% \n  st_drop_geometry()\n\n# Now do left_join safely:\ntree_density &lt;- tree_counts_no_geom %&gt;%\n  left_join(area_table, by = \"CounDist\") %&gt;%\n  mutate(density = n_trees / Shape_Area) %&gt;%\n  arrange(desc(density))\n\nhead(tree_density)\n\n\n# A tibble: 6 × 4\n  CounDist n_trees Shape_Area   density\n     &lt;int&gt;   &lt;int&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1       10    2943  76997844. 0.0000382\n2       16    1297  62082481. 0.0000209\n3        2     996  48322121. 0.0000206\n4       33    2106 110198297. 0.0000191\n5        3    1309  76315832. 0.0000172\n6       44    1693  99194858. 0.0000171\n\n\nDistrict 10 had the highest tree density!\nQuestion 3: Which district has the highest fraction of dead trees out of all trees?\n\n\nCode\ndead_fraction_by_district &lt;- tree_with_district %&gt;%\n  group_by(CounDist) %&gt;%\n  summarise(\n    total_trees = n(),\n    dead_trees = sum(tpcondition == \"Dead\", na.rm = TRUE), # adjust status column/value as needed\n    dead_fraction = dead_trees / total_trees\n  ) %&gt;%\n  arrange(desc(dead_fraction))\n\ndead_fraction_by_district\n\n\nSimple feature collection with 51 features and 4 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: -74.25448 ymin: 40.49891 xmax: -73.70139 ymax: 40.91292\nGeodetic CRS:  WGS 84\n# A tibble: 51 × 5\n   CounDist total_trees dead_trees dead_fraction                        geometry\n      &lt;int&gt;       &lt;int&gt;      &lt;int&gt;         &lt;dbl&gt;                &lt;MULTIPOINT [°]&gt;\n 1       23         785        271         0.345 ((-73.77803 40.72328), (-73.77…\n 2       22         237         81         0.342 ((-73.94214 40.76236), (-73.90…\n 3       32         754        254         0.337 ((-73.86115 40.57593), (-73.86…\n 4       21         148         47         0.318 ((-73.8558 40.73691), (-73.858…\n 5       26         278         88         0.317 ((-73.94083 40.7596), (-73.946…\n 6       29         385        121         0.314 ((-73.84204 40.68848), (-73.84…\n 7       27         467        142         0.304 ((-73.80742 40.70393), (-73.80…\n 8       30         586        173         0.295 ((-73.90304 40.704), (-73.9023…\n 9       24         560        158         0.282 ((-73.85075 40.73408), (-73.85…\n10       19        1064        291         0.273 ((-73.84464 40.77192), (-73.84…\n# ℹ 41 more rows\n\n\nCode\n# Highest fraction:\nmost_deadly &lt;- dead_fraction_by_district %&gt;% slice(1)\n\n\nDistrict 23 has the highest fraction of dead trees with 34.5% of trees in that district being dead.\n4: What is the most common tree species in Manhattan?\n\n\nCode\nlibrary(dplyr)\n\ntree_with_borough &lt;- tree_with_district %&gt;%\n  mutate(\n    borough = case_when(\n      CounDist &gt;= 1 & CounDist &lt;= 10 ~ \"Manhattan\",\n      CounDist &gt;= 11 & CounDist &lt;= 18 ~ \"Bronx\",\n      CounDist &gt;= 19 & CounDist &lt;= 32 ~ \"Queens\",\n      CounDist &gt;= 33 & CounDist &lt;= 48 ~ \"Brooklyn\",\n      CounDist &gt;= 49 & CounDist &lt;= 51 ~ \"Staten Island\",\n      TRUE ~ NA_character_\n    )\n  )\n\nmanhattan_trees &lt;- tree_with_borough %&gt;%\n  filter(borough == \"Manhattan\")\n\nmost_common_species &lt;- manhattan_trees %&gt;%\n  group_by(genusspecies) %&gt;%      \n  summarise(count = n()) %&gt;%\n  arrange(desc(count)) %&gt;%\n  slice(1)    # Get top species\n\nprint(most_common_species)\n\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: -74.01554 ymin: 40.70602 xmax: -73.90326 ymax: 40.87749\nGeodetic CRS:  WGS 84\n# A tibble: 1 × 3\n  genusspecies                                   count                  geometry\n  &lt;chr&gt;                                          &lt;int&gt;          &lt;MULTIPOINT [°]&gt;\n1 Gleditsia triacanthos var. inermis - Thornles…  1622 ((-73.99787 40.75982), (…\n\n\nGleditsia triacanthos var. inermis is the most common species in Manhattan! With 1622 trees in total.\n5: What is the species of trees closest to Baruch’s campus?\n\n\nCode\nnew_st_point &lt;- function(lat, lon, ...){\n    # st_sfc expects x, y which flips the normal lat (N/S) + lon (W/E) ordering\n    st_sfc(point = st_point(c(lon, lat))) |&gt;\n      st_set_crs(\"WGS84\")\n}\n\nbaruch_point &lt;- new_st_point(40.740366, -73.983064)\n\nlibrary(dplyr)\n\ntree_with_district %&gt;%\n  mutate(distance = as.numeric(st_distance(geometry, baruch_point))) %&gt;%\n  arrange(distance) %&gt;%\n  slice(1) %&gt;%\n  select(genusspecies, distance)\n\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -73.98417 ymin: 40.73936 xmax: -73.98417 ymax: 40.73936\nGeodetic CRS:  WGS 84\n                     genusspecies distance                   geometry\n1 Pyrus calleryana - Callery pear 145.7162 POINT (-73.98417 40.73936)\n\n\nCode\nclosest_tree_species &lt;- tree_with_district %&gt;%\n  mutate(distance = as.numeric(st_distance(geometry, baruch_point))) %&gt;%\n  arrange(distance) %&gt;%\n  slice(1) %&gt;%\n  pull(genusspecies)\n\nprint(closest_tree_species)\n\n\n[1] \"Pyrus calleryana - Callery pear\"\n\n\nPyrus Calleryana, known as the Callery Pear, is the species of tree most close to Baruch College at position (-73.98417, 40.73936)."
  },
  {
    "objectID": "mp03.html#task-5-replacing-dead-trees-in-council-district-23",
    "href": "mp03.html#task-5-replacing-dead-trees-in-council-district-23",
    "title": "STA 9750 Miniproject 3",
    "section": "Task 5: Replacing Dead Trees in Council District 23",
    "text": "Task 5: Replacing Dead Trees in Council District 23\nWe propose a focused project to replace dead street and park trees in Council District 23, enhancing neighborhood aesthetics, increasing canopy cover, supporting public health, and furthering New York City’s environmental sustainability goals. Our district has one of the highest fractions of dead trees in Queens, indicating an urgent need for investment.\nProject Scope:\n\nReplace: 271 dead or hazardous trees identified from the latest tree census\nRemove: 100 stumps to make way for new plantings\nPlant: 271 new, resilient trees, prioritizing canopy species and those most beneficial to local air quality\n\nCount the # of dead trees in District 23\n\n\nCode\nn_dead_d23 &lt;- tree_with_district %&gt;%\n  filter(CounDist == 23, tpcondition == \"Dead\") %&gt;%\n  nrow()\n\nprint(n_dead_d23)\n\n\n[1] 271\n\n\nCreate a map showcasing Dead Trees in NYC targeted for replacement.\n\n\nCode\nlibrary(ggplot2)\n\nggplot() +\n  geom_sf(data = filter(nycc_transformed, CounDist == 23), fill = NA, color = \"black\") +\n  geom_sf(data = filter(tree_with_district, CounDist == 23 & tpcondition == \"Dead\"), color = \"red\", size = 0.7, alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"Dead Trees in NYC Council District 23\", subtitle = \"Targeted for Replacement\")\n\n\n\n\n\n\n\n\n\nBar Graph comparing NYC District 23 with several other comparable districts (22, 21, 32)\n\n\nCode\nlibrary(dplyr)\n\n# Summarize tree stats by district\ntree_summary &lt;- tree_with_district %&gt;%\n  group_by(CounDist) %&gt;%\n  summarise(\n    total_trees = n(),\n    dead_trees = sum(tpcondition == \"Dead\", na.rm = TRUE),\n    dead_pct = dead_trees / total_trees\n  )\n\ndistrict_areas &lt;- tree_with_district %&gt;% select(CounDist, Shape_Area) %&gt;% st_drop_geometry()\n\ntree_cmp &lt;- tree_summary %&gt;%\n  left_join(district_areas, by = \"CounDist\") %&gt;%\n  mutate(\n    tree_density = total_trees / Shape_Area,\n    dead_density = dead_trees / Shape_Area\n  )\n\ncompare_districts &lt;- c(23, 22, 32, 21) # or others as you choose\n\ntree_cmp_selected &lt;- tree_cmp %&gt;%\n  filter(CounDist %in% compare_districts)\n\nlibrary(ggplot2)\nlibrary(scales)  # for percent()\n\nggplot(tree_cmp_selected, aes(x = factor(CounDist), y = dead_pct)) +\n  geom_col(fill = \"red\") +\n  geom_text(aes(label = percent(dead_pct, accuracy = 0.1)), vjust = -0.5) +\n  labs(\n    x = \"Council District\",\n    y = \"Percent Dead Trees\",\n    title = \"Proportion of Dead Trees in District 23 & Neighbors\"\n  ) +\n  scale_y_continuous(labels = percent_format(accuracy = 1), limits = c(0, 0.4))\n\n\nWarning: Removed 1920 rows containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\n\n\n\n\nDistrict 23 stands out as the most urgent candidate for a dead tree replacement initiative, with 34.5% of its trees classified as dead—the highest proportion among neighboring districts 21 (31.8%), 22 (34.2%), and 32 (33.7%). This elevated rate of tree loss signals a critical need for immediate investment, especially compared to surrounding areas, and suggests that targeted replanting here will deliver the greatest improvement in the local urban canopy, street safety, and neighborhood appearance.\n\n\nCode\nlibrary(ggplot2)\nlibrary(sf)\n\n# Select your districts\ndistricts_to_plot &lt;- c(23, 22)\n\n# Filter dead trees in those districts\nmap_dead_trees &lt;- tree_with_district %&gt;%\n  filter(CounDist %in% districts_to_plot, tpcondition == \"Dead\")\n\n# Get unique district boundaries (polygon geometry)\ndistrict_boundaries &lt;- tree_with_district %&gt;%\n  filter(CounDist %in% districts_to_plot)   # Use your polygons sf object\n\nggplot() +\n  geom_sf(data = map_dead_trees, color = \"red\", size = 1.5, alpha = 0.7) +\n  labs(\n    title = \"Dead Trees in Districts 23 and 22\",\n    subtitle = \"Red points show dead trees targeted for replacement\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe map clearly illustrates that District 23 not only has a high concentration of dead trees compared to its neighbor, but that these losses are spread across the community. By investing in the removal and replanting of dead trees in District 23, the Parks Department will address a clear and quantifiable gap—improving neighborhood safety, visual appeal, and environmental health. Overall, this project offers a data-driven opportunity to restore and revitalize our local urban forest where it is needed most."
  }
]